#!/usr/bin/env python3
"""
åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹çš„è„šæœ¬

ä½¿ç”¨æ–¹æ³•:
python merge_lora_weights.py \
    --base_model_path /path/to/base/model \
    --lora_weights_path /path/to/lora/weights \
    --output_path /path/to/merged/model
"""

import argparse
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig
import json


def merge_lora_weights(base_model_path, lora_weights_path, output_path, device_map="auto"):
    """
    å°†LoRAæƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­
    
    Args:
        base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„
        lora_weights_path: LoRAæƒé‡è·¯å¾„
        output_path: åˆå¹¶åæ¨¡å‹çš„è¾“å‡ºè·¯å¾„
        device_map: è®¾å¤‡æ˜ å°„ç­–ç•¥
    """
    print(f"æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
        device_map=device_map,
        low_cpu_mem_usage=True
    )
    
    print(f"æ­£åœ¨åŠ è½½LoRAæƒé‡: {lora_weights_path}")
    
    # åŠ è½½LoRAé…ç½®
    peft_config = PeftConfig.from_pretrained(lora_weights_path)
    
    # åˆ›å»ºPEFTæ¨¡å‹ï¼ˆåŠ è½½LoRAæƒé‡ï¼‰
    model = PeftModel.from_pretrained(
        base_model,
        lora_weights_path,
        torch_dtype=torch.bfloat16,
        device_map=device_map
    )
    
    print("æ­£åœ¨åˆå¹¶LoRAæƒé‡...")
    
    # åˆå¹¶æƒé‡å¹¶å¸è½½LoRAå±‚
    merged_model = model.merge_and_unload()
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_path, exist_ok=True)
    
    print(f"æ­£åœ¨ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ°: {output_path}")
    
    # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
    merged_model.save_pretrained(
        output_path,
        save_function=torch.save,
        safe_serialization=True
    )
    
    # åŠ è½½å¹¶ä¿å­˜tokenizer
    print("æ­£åœ¨ä¿å­˜tokenizer...")
    try:
        # é¦–å…ˆå°è¯•ä»LoRAæƒé‡ç›®å½•åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            lora_weights_path,
            trust_remote_code=True
        )
    except:
        # å¦‚æœLoRAç›®å½•æ²¡æœ‰tokenizerï¼Œä»åŸºç¡€æ¨¡å‹åŠ è½½
        tokenizer = AutoTokenizer.from_pretrained(
            base_model_path,
            trust_remote_code=True
        )
    
    tokenizer.save_pretrained(output_path)
    
    # ä¿å­˜æ¨¡å‹é…ç½®ä¿¡æ¯ï¼ˆç§»é™¤äº†æ—¥æœŸå­—æ®µï¼‰
    config_info = {
        "base_model": base_model_path,
        "lora_weights": lora_weights_path,
        "torch_dtype": "bfloat16",
        "merged": True
    }
    
    with open(os.path.join(output_path, "merge_info.json"), "w", encoding="utf-8") as f:
        json.dump(config_info, f, indent=2, ensure_ascii=False)
    
    print("âœ… LoRAæƒé‡åˆå¹¶å®Œæˆï¼")
    print(f"åˆå¹¶åçš„æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")
    
    # æ¸…ç†æ˜¾å­˜
    del base_model, model, merged_model
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


def verify_merged_model(model_path):
    """
    éªŒè¯åˆå¹¶åçš„æ¨¡å‹æ˜¯å¦å¯ä»¥æ­£å¸¸åŠ è½½
    
    Args:
        model_path: åˆå¹¶åæ¨¡å‹çš„è·¯å¾„
    """
    print(f"æ­£åœ¨éªŒè¯åˆå¹¶åçš„æ¨¡å‹: {model_path}")
    
    try:
        # å°è¯•åŠ è½½æ¨¡å‹å’Œtokenizer
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            trust_remote_code=True,
            device_map="cpu"  # åªç”¨CPUéªŒè¯ï¼Œé¿å…æ˜¾å­˜å ç”¨
        )
        
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        
        # ç®€å•çš„æ–‡æœ¬ç”Ÿæˆæµ‹è¯•
        test_input = "Hello, how are you?"
        inputs = tokenizer(test_input, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=10,
                do_sample=False,
                pad_token_id=tokenizer.eos_token_id
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"æµ‹è¯•ç”Ÿæˆ: {generated_text}")
        print("âœ… æ¨¡å‹éªŒè¯é€šè¿‡ï¼")
        
        # æ¸…ç†
        del model
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹éªŒè¯å¤±è´¥: {str(e)}")
        raise


def main():
    parser = argparse.ArgumentParser(description="åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹")
    parser.add_argument(
        "--base_model_path", 
        type=str, 
        required=True,
        help="åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆåŸå§‹é¢„è®­ç»ƒæ¨¡å‹ï¼‰"
    )
    parser.add_argument(
        "--lora_weights_path", 
        type=str, 
        required=True,
        help="LoRAæƒé‡è·¯å¾„ï¼ˆSFTè®­ç»ƒè¾“å‡ºçš„lora_weightsç›®å½•ï¼‰"
    )
    parser.add_argument(
        "--output_path", 
        type=str, 
        required=True,
        help="åˆå¹¶åæ¨¡å‹çš„è¾“å‡ºè·¯å¾„"
    )
    parser.add_argument(
        "--device_map", 
        type=str, 
        default="auto",
        help="è®¾å¤‡æ˜ å°„ç­–ç•¥ (auto, cpu, cuda, ç­‰)"
    )
    parser.add_argument(
        "--verify", 
        action="store_true",
        help="æ˜¯å¦éªŒè¯åˆå¹¶åçš„æ¨¡å‹"
    )
    parser.add_argument(
        "--force", 
        action="store_true",
        help="å¦‚æœè¾“å‡ºç›®å½•å·²å­˜åœ¨ï¼Œæ˜¯å¦å¼ºåˆ¶è¦†ç›–"
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥è·¯å¾„
    if not os.path.exists(args.base_model_path):
        raise ValueError(f"åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.base_model_path}")
    
    if not os.path.exists(args.lora_weights_path):
        raise ValueError(f"LoRAæƒé‡è·¯å¾„ä¸å­˜åœ¨: {args.lora_weights_path}")
    
    # æ£€æŸ¥è¾“å‡ºè·¯å¾„
    if os.path.exists(args.output_path) and not args.force:
        if os.listdir(args.output_path):  # ç›®å½•éç©º
            response = input(f"è¾“å‡ºç›®å½• {args.output_path} å·²å­˜åœ¨ä¸”éç©ºï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ(y/N): ")
            if response.lower() != 'y':
                print("æ“ä½œå·²å–æ¶ˆ")
                return
    
    print("=== LoRAæƒé‡åˆå¹¶å¼€å§‹ ===")
    print(f"åŸºç¡€æ¨¡å‹: {args.base_model_path}")
    print(f"LoRAæƒé‡: {args.lora_weights_path}")
    print(f"è¾“å‡ºè·¯å¾„: {args.output_path}")
    print(f"è®¾å¤‡æ˜ å°„: {args.device_map}")
    print("========================")
    
    # æ‰§è¡Œåˆå¹¶
    try:
        merge_lora_weights(
            base_model_path=args.base_model_path,
            lora_weights_path=args.lora_weights_path,
            output_path=args.output_path,
            device_map=args.device_map
        )
        
        # éªŒè¯æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        if args.verify:
            verify_merged_model(args.output_path)
            
    except Exception as e:
        print(f"âŒ åˆå¹¶è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        raise
    
    print("\nğŸ‰ æ‰€æœ‰æ“ä½œå®Œæˆï¼åˆå¹¶åçš„æ¨¡å‹å¯ä»¥ç”¨äºGRPOè®­ç»ƒæˆ–æ¨ç†ã€‚")


if __name__ == "__main__":
    main()
